## I want all of my JVM projects to use the same build logic

> *Dear JVM Guy, I'm an architect supporting a solution based on multiple cooperating JVM projects and have noticed that
> each team builds their software in a unique fashion.  One team's build will run automated tests and detect a lack of
> code coverage but another team's build just assembles the WAR file and lets QA find the bugs.  I realize that a chain
> is only as strong as its weakest link and was wondering what can I do to unify the builds of each project?*
>
> *Anonymous Architect*

**The JVM Guy Recommends: [Gradle](http://gradle.org/) and [Docker](https://www.docker.com/)**

Dear Anonymous, you didn't mention the programming languages that your project is using or the build tool(s) so I'll
base my recommendation assuming that Java, Groovy and Scala might be in play.  First off, I think your primary issue is
a people problem, not a technology problem. Somehow you are going to have get your teams to agree on a common level of
quality for all the cooperating pieces -- which is not an easy task. Assuming that you can get the teams all reading
from the same page, then you can move on to some technological solutions.  My first recommendation is a no brainer and
is common in many projects: Gradle.  If you haven't already replaced your Ant and Maven builds with Gradle, then work
towards doing that straight away. Gradle has matured over the past couple of years and you can find some very well
written books describing the ins and outs of the tool.  The heart of Gradle is its plugin mechanism that allows for you
to declare the type of project you are building and let Gradle figure out all the details and manage the sequence of
steps.  For production systems, I recommend that a build perform at least the following steps:

* compile source into binaries
* assemble binaries into artifacts (JARs of production bits, production source, test bits, test source, API
documentation)
* execute unit-level tests (remember, unit tests are fast and **do not require the network, filesystem or other objects
to work**)
* verify that each object has a unit-level test in place
* execute component-level tests (multiple cooperating objects being tested together but not requiring the entire
application to be stood up)
* execute integration-level tests (the entire application stood up and tested in quasi-production environment)
* publishes artifacts that have passed the testing phase to a repository on the network

All of these steps are well documented in the Gradle literature and do not need any discussion here.  What isn't
commonly
discussed, however, is how to get each team to use Gradle in the same manner, using the same plugins, adhering to the
same thresholds and applying the same corporate logic and settings.  For discussion, let us assume that our only goal
is to get each project to apply the same set of plugins:

{title="Desired Gradle Plugins", lang="Groovy"} 
~~~~~~~~ 
apply plugin: 'java'
apply plugin: 'groovy'
apply plugin: 'scala'
apply plugin: 'maven-publish'
apply plugin: 'checkstyle'
apply plugin: 'codenarc'
apply plugin: 'jacoco'
apply plugin: 'sonar-runner'
~~~~~~~~ 

T> ## Applying Real-world Build Logic
T> In a real world scenario we would detect what plugins were applied by the project and conditionally apply additional
T> plugins and settings.  We might also fail the build if a required plugin was not applied.  For example,
T> 
T> {title="Conditional Gradle Logic Based On Detected Plugin", lang="Groovy"} 
T> ~~~~~~~~ 
T> if ( plugins.hasPlugin( JavaPlugin ) ) { 
T>     logger.quiet 'Java plugin detected!
T>     compileJava.options*.compilerArgs = ['-Xlint:deprecation']
T> 
T>     logger.quiet 'Applying CheckStyle plug-in'
T>     plugins.apply( 'checkstyle' )
T>     properties['checkstyle'].ignoreFailures = false
T>     checkstyleMain.enabled = true
T>     checkstyleTest.enabled = true
T> }
T> ~~~~~~~~ 
T> 

So, I can think of 3 ways to get teams to share Gradle logic:

* use a lazybones template and have each project start with a common Gradle setup, customizing it as needed
* use a customized Gradle distribution pulled from the corporate network
* use Docker to encapsulate Gradle and the custom logic it applies

Each, as always, has its benefits and drawbacks.

### Each Project Has Its Own Gradle Setup
Using lazybones to stamp out a common Gradle set up is very simple to do and gives each new project a common place to
start. An example multi-project build might look like this:

{title="Multi-project Gradle Build Based On A Template"} 
~~~~~~~~ 
├── build.gradle
├── config
│   ├── checkstyle.xml
│   └── codenarc.xml
├── gradle
│   ├── application.gradle
│   ├── boot.gradle
│   ├── checkstyle.gradle
│   ├── codenarc.gradle
│   ├── findbugs.gradle
│   ├── groovyCompilerConfiguration.groovy
│   ├── groovy.gradle
│   ├── jacoco.gradle
│   ├── jdepend.gradle
│   ├── maven-publish.gradle
│   ├── pmd.gradle
│   ├── shell.gradle
│   ├── war.gradle
│   └── wrapper
│       ├── gradle-wrapper.jar
│       └── gradle-wrapper.properties
├── gradle.properties
├── gradlew
├── gradlew.bat
├── overview.html
├── README.md
├── service
│   ├── build.gradle
│   └── src
│       ├── main
│       │   ├── groovy
│       │   │   └── org
│       │   │       └── example
│       │   │           ├── Application.groovy
│       │   ├── resources
│       │   │   ├── banner.txt
│       │   │   ├── config
│       │   │   │   ├── application-production.properties
│       │   │   │   ├── application.properties
│       │   │   │   └── application-test.properties
│       │   │   └── logback.xml
│       └── test
│           ├── groovy
│           │   └── org
│           │       └── example
│           │           ├── ApiAcceptanceTest.groovy
│           │           └── shared
│           │               ├── BaseComponentTest.groovy
│           │               ├── BaseSpecification.groovy
│           │               ├── BaseStepDefinition.groovy
│           │               ├── BaseUnitTest.groovy
│           └── resources
│               ├── logback-test.xml
│               └── org
│                   └── example
│                       └── echo
│                           ├── EchoService.feature
│                           └── Operations.feature
├── settings.gradle
├── shared
│   ├── build.gradle
│   └── src
│       └── main
│           ├── config
│           │   └── checkstyle-suppressions.xml
│           ├── groovy
│           │   └── org
│           │       └── example
│           │           └── echo
│           │               ├── EchoRequest.groovy
│           │               └── EchoResponse.groovy
│           └── java
│               └── org
│                   └── example
│                       └── shared
│                           └── resilience
│                               ├── HystrixSettingsBuilder.java
│                               └── package-info.java
├── shell
│   ├── build.gradle
│   └── src
│       └── main
│           ├── groovy
│           │   └── org
│           │       └── example
│           │           └── shell
│           │               ├── Application.groovy
│           │               ├── ApplicationProperties.groovy
│           │               ├── configuration
│           │               │   ├── BannerProvider.groovy
│           │               │   ├── HistoryFileNameProvider.groovy
│           │               │   └── PromptProvider.groovy
│           │               └── shared
│           │                   ├── BaseCommand.groovy
│           │                   └── RabbitGateway.groovy
│           └── resources
│               ├── config
│               │   └── application.properties
│               ├── logback.xml
│               └── META-INF
│                   └── spring
│                       └── spring-shell-plugin.xml
~~~~~~~~ 

Q> ## Why have a multi-project build for a single application?
Q>
Q> Over time I've come to appreciate the usefulness of having a companion command-line program to complement that actual 
Q> application I'm building. In the past, whenever I wanted to try out some sort of experiment I would hack an existing 
Q> integration test to set up the conditions of the experiment. Now I use Spring Shell to build a simple CLI that I can
Q> use to drive those experiments.  If I need to drop 100,000 messages on a queue, I just use my cli tool. If I want to
Q> see what happens when I send a 1 GB file 100 times in a row, I just use my cli tool.
Q>

T> ## Keeping Gradle files small and lean
T> In the file structure above, you'll notice that the `gradle` folder contains lots of `.gradle` files.  I like to
T> split up logic for each plugin into its own file and include that into the build file.  I find that it makes it
T> easier to reuse pieces and keeps the main build file small and easier to understand.
T> 
T> {title="war.gradle", lang="Groovy"} 
T> ~~~~~~~~ 
T> apply plugin: 'war'
T> 
T> war.manifest.attributes( "Implementation-Title": project.name, 
T>                          "Implementation-Version": project.version )
T> ~~~~~~~~ 
T> 
T> {title="service/build.gradle", lang="Groovy"} 
T> ~~~~~~~~ 
T> description = 'Example micro-service implementation'
T> 
T> apply from: "$rootDir/gradle/war.gradle"
T> ~~~~~~~~ 
T> 

The primary benefits of having each project manage its own build system is that it is simple to do and each team can
evolve its own set of best practices.  Each team can migrate to newer versions of Gradle and the JDK at its own pace
and not have to move in lock-step with other projects.  The downside here is that there is no governance.  Unless you
periodically police the build files, you really won't know what each team is doing and monitor any drift that may occur.

T> ## Use [SonarQube](http://www.sonarqube.org/) to track trends
T> One solution to the governance problem is to mandate that each project publish data to SonarQube.  You can then use 
T> SonarQube to guage how each project is doing in relation to the others.  Requiring SonarQube be continually updated
T> gives you the data you need to get a feel for the quality of each project and decide if any corrective actions need
T> to be taken. If you decide to go this route **make sure you look at the data on a consistent basis.** I've worked on
T> projects that published data nobody looked at.  This is a waste of resources and can be disheartening to teams that
T> are already suspect of corporate mandates and policies.
T> 

### Use A Custom Gradle Distribution
A Gradle best practice is not to require Gradle be installed on a machine before it can execute a build.  Instead, use a 
bootstrap script that downloads the required Gradle distribution from the network.  This bootstrap script can point to a
custom distribution on the corporate network.  The blog post
[Gradle Goodness: Distribute Custom Gradle in Our Company](http://mrhaki.blogspot.com/2012/10/gradle-goodness-distribute-custom.html) 
does a good job of explaining how to wire that up. What it doesn't explain is how to take advantage of the custom
packaging. What I've done in the past is to create an `init.gradle` file that contains all the custom logic that we
wanted to share between projects. Placing this file into the `init.d` folder of the distribution ensures that the custom
logic gets automatically applied.  As in the above example, conditional logic is applied based on plugins that project's
build file applies on its own.

{title="Snippet of an init.d/init.gradle file", lang="Groovy"} 
~~~~~~~~ 
logger.quiet 'Running custom init.gradle file'

buildscript {
    allprojects { 
        // specify corporate repositories
        // conditionally apply plugins and configurations
    } 
}
~~~~~~~~ 

Each project needs to use the `Gradle Wrapper` mechanism so that the corporate distribution is used instead of the
standard one. The wrapper mechanism is well documented and easy to set up.  You will, however, need to make a minor edit
so that the wrapper script uses your custom distribution instead of the off-the-shelf one.  Edit your
`gradle/wrapper/gradle-wrapper.properties` so that it points to your distribution.

{title="Example gradle/wrapper/gradle-wrapper.properties"} 
~~~~~~~~ 
#Thu Jul 24 01:31:07 EDT 2014
distributionBase=GRADLE_USER_HOME
distributionPath=wrapper/dists
zipStoreBase=GRADLE_USER_HOME
zipStorePath=wrapper/dists
distributionUrl=https\://my.corporation.com/distributions/gradle-1.2.3-all.zip
~~~~~~~~ 

Setting up a custom distribution isn't hard but it isn't without its drawbacks. First, whenever Gradle releases a new
version, you are going to have to publish an updated custom distribution.  Once the distribution is updated, each
project will have to be updated to point to the new distribution.  This is a minor inconvenience at worst and is what
would have to be done when upgrading to a newer version of Gradle anyways.  Moving from Gradle 1.2 to 1.3 isn't a big
deal and not very risky.  What can happen, however, in a custom distribution is that thresholds and requirements can
change between releases and break builds. For example, if it is mandated that the new policy is to have 60% unit test
coverage instead of the old 50%, builds will break until the new tests are written.  Some changes can have a huge
impact on existing code bases as well.  For example, if the old policy was to not run Checkstyle on test code but the
new policy is to have test code inspected, builds will be broken until the violations are corrected.  In short, if the
thresholds and policies don't change that often a custom distribution is a reasonable way to go.  If things change
often, however, then teams are likely to get frustrated and either not move forward with versions or revert back to
their own build system.

### Abstracting Away Build Details Using Docker
So far, we've been focusing on the build from the perspective of the developer.  If we switch focus to that of the build 
server, we can see that several pieces have to bee in place in order for a CI server to successfully produce a build.
For example, the proper JDK version must present.  Depending on the build technology being used, the proper versions of
Ant, Maven or Gradle must also be installed.  Hostname and SSH keys might have to set.  There are a fair amount of ducks
to get into a row before a build can be produced.  What if we could keep our CI servers "dumb" and move the smarts to
the build itself?  What if the CI server didn't know, or care, if Gradle, Maven or Ant was being used to drive the
build?  At the very least, that would greatly lessen the burden of configuring the CI cluster when a new project came
online.  More interesting is that your build server could "go back in time" and recreate an **exact** environment,
including build tooling, of a build months or years in the past, allowing you to make that emergency patch for your
most important customer.  The secret ingredient to such a solution is Docker.

Docker is the new hotness and is being used in a variety of ways.  Its primary capabilitiy is that it allows you to
bundle up an entire environment into a package and run it isolated from other processes.  Consider it a very lightweight
form of virtualization.  Typically, the Docker community focuses on how to use Docker in production settings.  Here, we
will focus on using it in a CI pipeline.  The best way to discuss the process is via example.  Let us assume we have two
projects.  One is built using Ant and one using Gradle.  Further, let's assume both projects are using our Docker based
build process.  On the CI server,the following command is used to build the Ant project:

{title="Command to build an Ant-based project", lang="Bash"} 
~~~~~~~~ 
./assemble-artifacts.sh
~~~~~~~~ 

For our Gradle based project, the CI server runs this command:

{title="Command to build a Gradle-based project", lang="Bash"} 
~~~~~~~~ 
./assemble-artifacts.sh
~~~~~~~~ 

You are probably thinking to yourself that the shell script holds all the installation and build magic and Docker isn't
really necessary. Let's take a look under the hood and see if you are right:

{title="Ant-based project's assemble-artifacts.sh", lang="Bash"} 
~~~~~~~~ 
#!/bin/bash

docker run --volume $(pwd):/src 
           --workdir=/src 
           --user=$(id -u $(whoami)):$(id -g $(whoami)) 
           --rm 
           --tty
           --interactive
           jvm-guy/ant-build:1.0.0 "$@"
~~~~~~~~ 

{title="Gradle-based project's assemble-artifacts.sh", lang="Bash"} 
~~~~~~~~ 
#!/bin/bash

docker run --volume $(pwd):/src 
           --workdir=/src 
           --user=$(id -u $(whoami)):$(id -g $(whoami)) 
           --rm 
           --tty
           --interactive
           jvm-guy/gradle-build:1.0.0 "$@"
~~~~~~~~ 

Each script simply runs a docker command with a particular set of switches. There is a lot going on in the command but
the really important parts are `jvm-guy/ant-build:1.0.0` and `jvm-guy/gradle-build:1.0.0`.  Those are references to
particular versions of Docker images available on the network.

I> ## What is a Docker image?
I>
I> An image is a binary artifact that contains all the software and environmental settings needed to run one or more
I> programs in an isolated environment. For example, the `jvm-guy/gradle-build:1.0.0` image might have the following
I> installed and configured:
I>
I> * Oracle JDK 1.8.25
I> * Gradle version 2.2.1
I> * JAVA_HOME set to the location of the JDK
I> * GRADLE_HOME set to the location of the Gradle installation
I>
I> The `jvm-guy/ant-build:1.0.0` might have the following installed and configured:
I>
I> * Open JDK 1.7.45
I> * Apache Ant version 1.9.4
I> * JAVA_HOME set to the location of the JDK
I> * ANT_HOME set to the location of the Ant installation
I>
I> Docker images, once built, can be versioned and shared over the network.
I>

Another important part of the Docker command is the `--volume` switch.   This says that the current working directory
should be visible inside the container as `/src`.  This gives the container the ability to read and write files on the
local file system. In a nutshell, the Ant script is saying "Hey, Docker.  Run Ant against the files in the current
directory and write the output to there as well".  The Gradle script is saying something similar, just substituting
Gradle for Ant.  What is nice about using Docker to manage builds is that the machine doing the build can be "bare"
meaning that none of the usual software has to be installed on the machine. No JDKs. No Ant. No Maven. No Gradle.
No Ruby. No Python.  Only the CI software.  No more version conflicts to deal with.  Nice.

I> ## Docker images are 'layered'
I>
I> Docker images are built upon other images, encouraging reuse.  You will typically find that images are built on
I> refinements of a  series of upstream images. For example, I might build an image that installs JDK 8 and nothing
I> else.  I might then build another image, based on the JDK 8 image, that installs Gradle.  A third image, based on
I> the Gradle image, might install a generic  Gradle build file.
I>

To give you an idea of the lineage of the images, here are their Docker files (Docker's version of a build file):

{title="JDK 8 Dockerfile", lang="Bash"} 
~~~~~~~~ 
# Use Ubuntu as our base image 
FROM ubuntu:14.04

# Install JDK 8 
RUN apt-get --quiet update && 
                    apt-get --quiet --yes install wget && 
                    apt-get clean && 
                    wget --quiet 
                         --output-document=/jdk-8.tar.gz 
                         --no-check-certificate 
                         --no-cookies 
                         --header "Cookie: oraclelicense=accept-securebackup-cookie" 
                         http://download.oracle.com/otn-pub/java/jdk/8u25-b17/jdk-8u25-linux-x64.tar.gz  && 
                    mkdir -p /usr/lib/jvm && 
                    tar --gunzip --extract --verbose --file /jdk-8.tar.gz --directory /usr/lib/jvm && 
                    rm -f /jdk-8.tar.gz && 
                    chown -R root:root /usr/lib/jvm 

# set the environment variables 
ENV JDK_HOME /usr/lib/jvm/jdk1.8.0_25 
ENV JAVA_HOME /usr/lib/jvm/jdk1.8.0_25
ENV PATH $PATH:$JAVA_HOME/bin
~~~~~~~~ 

{title="Gradle Dockerfile", lang="Bash"} 
~~~~~~~~ 
# Use Java 8 image as our base 
FROM jvmguy/java:1.8.0.25

# install Gradle 
RUN apt-get --quiet --yes install unzip && 
                                  apt-get clean && 
                                  wget --quiet 
                                       --output-document=/gradle.zip https://services.gradle.org/distributions/gradle-2.2-all.zip && 
                                  mkdir -p /usr/lib/jvm && 
                                  unzip /gradle.zip -d /usr/lib/jvm && 
                                  rm -f /gradle.zip && 
                                  chown -R root:root /usr/lib/jvm 

# set the environment variables 
ENV GRADLE_HOME /usr/lib/jvm/gradle-2.2 
ENV PATH $PATH:$GRADLE_HOME/bin
ENV GRADLE_USER_HOME  /src/.cache/gradle

# create a mount point where the source files will be
VOLUME ["/src"]

ENTRYPOINT [ "gradle"]
CMD ["--info"]
~~~~~~~~ 

{title="Custom Gradle Build Dockerfile", lang="Bash"} 
~~~~~~~~ 
# Use Gradle image as our base 
FROM jvmguy/gradle:2.2.0 

# copy the Gradle files that will control the build sequence
COPY *.gradle /usr/lib/jvm/gradle-2.2/init.d/
 
# entrypoint is inherited from the gradle image
~~~~~~~~ 

From the perspective of the CI server, the benefit of a Docker-based build system is easu to see -- no more installation
of build tools or runtimes.  From the developer's perspective, the burden of writing and maintaining a build system has
been off-loaded to somebody else.  The developer just puts the code in expected places and runs the version of the build
tool needed for that particular project.  Obviously, the developer has to know something about the build tool, such as
how to indicate that a particular file should be excluded from code coverage analysis, but the majority of the build
logic is encapsulated away inside the Docker container.  I haven't seen this technique used in many places but I expect
it to become more common place because the benefit is too great to ignore. A project becomes a lot less build logic and
a lot more code.  People specializing in the current build tools can maintain the containers and advertise them to the
teams as new containers are released. Projects become more standardized and predictable without having to sacrifice
developer time each month when a new version of some build tool comes out.  I wish I could say that I know this will
work for everybody but I can't.  What I can recommend, however, is that you take a hard look at using Docker to separate
your build logic away from your development team.
